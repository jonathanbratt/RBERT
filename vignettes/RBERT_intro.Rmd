---
title: "Introduction to RBERT"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to RBERT}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<!--  Copyright 2019 Bedford Freeman & Worth Pub Grp LLC DBA Macmillan Learning. -->
<!--  -->
<!--  Licensed under the Apache License, Version 2.0 (the "License"); -->
<!--  you may not use this file except in compliance with the License. -->
<!--  You may obtain a copy of the License at -->
<!--  -->
<!--      http://www.apache.org/licenses/LICENSE-2.0 -->
<!--  -->
<!--  Unless required by applicable law or agreed to in writing, software -->
<!--  distributed under the License is distributed on an "AS IS" BASIS, -->
<!--  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. -->
<!--  See the License for the specific language governing permissions and -->
<!--  limitations under the License. -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```





RBERT is an implementation of Google Research's 
[BERT](https://github.com/google-research/bert) in `R`. 

BERT is a powerful general-purpose language model
(paper [here](https://arxiv.org/pdf/1810.04805.pdf), 
helpful blog post [here](http://jalammar.github.io/illustrated-bert/)).
BERT is written in Python, using [TensorFlow](https://www.tensorflow.org/).
An `R` package for TensorFlow already [exists](https://tensorflow.rstudio.com/),
so the goal of this project is to fully implement BERT in `R` down to the level
of the TensorFlow API.

Generally speaking, there are three levels at which BERT could be used:

1. Using the output of a pre-trained BERT model as features for downstream model
2. Fine-tuning on top of a pre-trained BERT model
3. Training a BERT model from scratch

Currently, RBERT is functional at the first level, and possibly functional
at the second level (speed becomes a significant consideration at this level).

# Getting started with RBERT

RBERT requires the tensorflow package to be installed and working. If that
requirement is met, using RBERT at the first level is fairly 
straightforward. 

```{r eval = FALSE}
library(RBERT)
temp_dir <- tempdir()
# Download pre-trained BERT model.
RBERT::download_BERT_checkpoint(model = "bert_base_uncased",
                                destination = temp_dir)

# path to downloaded BERT checkpoint
BERT_PRETRAINED_DIR <- file.path(temp_dir,
                                 "BERT_checkpoints", 
                                 "uncased_L-12_H-768_A-12")
vocab_file <- file.path(BERT_PRETRAINED_DIR, 'vocab.txt')
init_checkpoint <- file.path(BERT_PRETRAINED_DIR, 'bert_model.ckpt')
bert_config_file <- file.path(BERT_PRETRAINED_DIR, 'bert_config.json')

text_to_process <- c("Impulse is equal to the change in momentum.",
                     "Changing momentum requires an impulse.",
                     "An impulse is like a push.",
                     "Impulse is force times time.")

BERT_feats <- RBERT::extract_features(
  examples = RBERT::make_examples_simple(text_to_process),
  vocab_file = vocab_file,
  bert_config_file = bert_config_file,
  init_checkpoint = init_checkpoint,
  layer_indexes = as.list(1:12),
  batch_size = 2L
)

# Extract the final layer output vector for the "[CLS]" token of the first
# sentence. 
output_vector1 <- BERT_feats$layer_outputs$example_1$features$token_1$layers$layer_output_12$values

# Extract output vectors for all sentences...
# These vectors can be used as input features for downstream models.
# Convenience functions for doing this extraction will be added to the
# package in the near future.
output_vectors <- purrr::map(BERT_feats$layer_outputs, function(sentence) {
  sentence$features$token_1$layers$layer_output_12$values
})

# Clean up.
unlink(temp_dir, recursive = TRUE)
```

# Future work

There's still a lot to do! Check out the 
[issues board](https://github.com/macmillanhighered/RBERT/issues) 
on the github page.
