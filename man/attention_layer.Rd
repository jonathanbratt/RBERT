% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/modeling.R
\name{attention_layer}
\alias{attention_layer}
\title{Build multi-headed attention layer}
\usage{
attention_layer(
  from_tensor,
  to_tensor,
  attention_mask = NULL,
  num_attention_heads = 1L,
  size_per_head = 512L,
  query_act = NULL,
  key_act = NULL,
  value_act = NULL,
  attention_probs_dropout_prob = 0,
  initializer_range = 0.02,
  do_return_2d_tensor = FALSE,
  batch_size = NULL,
  from_seq_length = NULL,
  to_seq_length = NULL
)
}
\arguments{
\item{from_tensor}{Float Tensor of shape \code{[batch_size, from_seq_length,
from_width]}.}

\item{to_tensor}{Float Tensor of shape \code{[batch_size, to_seq_length,
to_width]}.}

\item{attention_mask}{(optional) Integer Tensor of shape \code{[batch_size,
from_seq_length, to_seq_length]}. The values should be 1 or 0. The
attention scores will effectively be set to -infinity for any positions in
the mask that are 0, and will be unchanged for positions that are 1.}

\item{num_attention_heads}{Integer; number of attention heads.}

\item{size_per_head}{Integer; size of each attention head.}

\item{query_act}{(Optional) Activation function for the query transform.}

\item{key_act}{(Optional) Activation function for the key transform.}

\item{value_act}{(Optional) Activation function for the value transform.}

\item{attention_probs_dropout_prob}{(Optional) Numeric; dropout probability
of the attention probabilities.}

\item{initializer_range}{Numeric; range of the weight initializer.}

\item{do_return_2d_tensor}{Logical. If TRUE, the output will be of shape
\code{[batch_size * from_seq_length, num_attention_heads * size_per_head]}.
If false, the output will be of shape \code{[batch_size, from_seq_length,
num_attention_heads * size_per_head]}.}

\item{batch_size}{(Optional) Integer; if the input is 2D, this might (sic) be
the batch size of the 3D version of the \code{from_tensor} and
\code{to_tensor}.}

\item{from_seq_length}{(Optional) Integer; if the input is 2D, this might be
the seq length of the 3D version of the \code{from_tensor}.}

\item{to_seq_length}{(Optional) Integer; if the input is 2D, this might be
the seq length of the 3D version of the \code{to_tensor}.}
}
\value{
float Tensor of shape \code{[batch_size, from_seq_length,
  num_attention_heads * size_per_head]}. If \code{do_return_2d_tensor} is
  TRUE, it will be flattened to shape \code{[batch_size * from_seq_length,
  num_attention_heads * size_per_head]}.
}
\description{
Performs multi-headed attention from \code{from_tensor} to \code{to_tensor}.
This is an implementation of multi-headed attention based on "Attention is
all you Need". If \code{from_tensor} and \code{to_tensor} are the same, then
this is self-attention. Each timestep in \code{from_tensor} attends to the
corresponding sequence in \code{to_tensor}, and returns a fixed-with vector.
This function first projects \code{from_tensor} into a "query" tensor and
\code{to_tensor} into "key" and "value" tensors. These are (effectively) a
list of tensors of length \code{num_attention_heads}, where each tensor is of
shape \code{[batch_size, seq_length, size_per_head]}. Then, the query and key
tensors are dot-producted and scaled. These are softmaxed to obtain attention
probabilities. The value tensors are then interpolated by these
probabilities, then concatenated back to a single tensor and returned.
}
\details{
In practice, the multi-headed attention are done with transposes and reshapes
rather than actual separate tensors.
}
\examples{
\dontrun{
# Maybe add examples later. For now, this is only called from
# within transformer_model(), so refer to that function.
}
}
