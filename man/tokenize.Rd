% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenization.R
\name{tokenize}
\alias{tokenize}
\alias{tokenize.FullTokenizer}
\alias{tokenize.BasicTokenizer}
\alias{tokenize.WordpieceTokenizer}
\title{Tokenizers for various objects.}
\usage{
tokenize(tokenizer, text)

\method{tokenize}{FullTokenizer}(tokenizer, text)

\method{tokenize}{BasicTokenizer}(tokenizer, text)

\method{tokenize}{WordpieceTokenizer}(tokenizer, text)
}
\arguments{
\item{tokenizer}{The Tokenizer object to refer to.}

\item{text}{The text to tokenize. For tokenize.WordpieceTokenizer, the text
should have already been passed through BasicTokenizer.}
}
\value{
A list of tokens.
}
\description{
This tokenizer performs some basic cleaning, then splits up text on
whitespace and punctuation.
}
\section{Methods (by class)}{
\itemize{
\item \code{FullTokenizer}: Tokenizer method for objects of FullTokenizer class.

\item \code{BasicTokenizer}: Tokenizer method for objects of BasicTokenizer class.

\item \code{WordpieceTokenizer}: Tokenizer method for objects of WordpieceTokenizer
class. This uses a greedy longest-match-first algorithm to perform
tokenization using the given vocabulary. For example: input = "unaffable"
output = list("un", "##aff", "##able") ... although, ironically, the BERT
vocabulary actually gives output = list("una", "##ffa", "##ble") for this
example, even though they use it as an example in their code.
}}

\examples{
\dontrun{
tokenizer <- FullTokenizer("vocab.txt", TRUE)
tokenize(tokenizer, text = "a bunch of words")
}
\dontrun{
tokenizer <- BasicTokenizer(TRUE)
tokenize(tokenizer, text = "a bunch of words")
}
\dontrun{
vocab <- load_vocab(vocab_file = "vocab.txt")
tokenizer <- WordpieceTokenizer(vocab)
tokenize(tokenizer, text = "a bunch of words")
}
}
