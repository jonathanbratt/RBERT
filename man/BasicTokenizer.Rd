% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenization.R
\name{BasicTokenizer}
\alias{BasicTokenizer}
\title{Construct objects of BasicTokenizer class.}
\usage{
BasicTokenizer(do_lower_case = TRUE)
}
\arguments{
\item{do_lower_case}{Logical; the value to give to the "do_lower_case"
argument in the BasicTokenizer object.}
}
\value{
an object of class BasicTokenizer
}
\description{
(I'm not sure that this object-based approach is best for R implementation,
but for now just trying to reproduce python functionality.)
}
\details{
Has methods: `tokenize.BasicTokenizer()` `run_strip_accents.BasicTokenizer()`
(internal use) `run_split_on_punc.BasicTokenizer()` (internal use)
`tokenize_chinese_chars.BasicTokenizer()` (internal use)
`is_chinese_char.BasicTokenizer()` (internal use)
`clean_text.BasicTokenizer()` (internal use)
}
\examples{
\dontrun{
b_tokenizer <- BasicTokenizer(TRUE)
}
}
