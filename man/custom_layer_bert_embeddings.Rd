% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tf2-layer_embeddings.R
\name{custom_layer_bert_embeddings}
\alias{custom_layer_bert_embeddings}
\title{Custom Layer: Bert Embeddings}
\usage{
custom_layer_bert_embeddings(
  object,
  name = NULL,
  trainable = NULL,
  param_list = list(),
  ...
)
}
\arguments{
\item{object}{Model or layer object.}

\item{name}{Character; An optional name for the layer. Must be unique in a
model.}

\item{trainable}{Logical; whether the layer weights will be updated during
training.}

\item{param_list}{A named list of parameter values used in defining the
layer. Some parameters for the BERT model are:
\describe{
\item{\code{num_layers}}{Integer; the number of attention layers to create.}
\item{\code{num_heads}}{Integer; the number of attention heads per layer.}
\item{\code{hidden_size}}{Integer; the size of the embedding vectors.}
\item{\code{hidden_dropout}}{Dropout probability to apply to dense layers...}
\item{\code{attention_dropout}}{Dropout probability to apply to attention... }
\item{\code{intermediate_size}}{Integer; size of dense layers after self
attention mechanism.}
\item{\code{intermediate_activation}}{Character; activation function to use with
dense layers... }
\item{\code{vocab_size}}{Integer; number of tokens in vocabulary.}
\item{\code{token_type_vocab_size}}{Integer; number of input segments that the
model will recognize. (Two for BERT models.) }
\item{\code{max_position_embeddings}}{Integer; maximum number of tokens in
input.}
\item{\code{embedding_size}}{(Used for ALBERT; NULL for BERT.) }
\item{\code{shared_layer}}{(TRUE for ALBERT, FALSE for BERT.) }
}}
}
\description{
Create embeddings layer for BERT model. Embeddings (may) include token type
and position embeddings components as well as token embeddings. All
embeddings components have the same dimension, and are simply added together.
}
