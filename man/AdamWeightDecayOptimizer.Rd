% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimization.R
\name{AdamWeightDecayOptimizer}
\alias{AdamWeightDecayOptimizer}
\title{Constructor for objects of class AdamWeightDecayOptimizer}
\usage{
AdamWeightDecayOptimizer(learning_rate, weight_decay_rate = 0,
  beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-06,
  exclude_from_weight_decay = NULL, name = "AdamWeightDecayOptimizer")
}
\arguments{
\item{learning_rate}{Numeric Tensor (single element?); learning rate.}

\item{weight_decay_rate}{Numeric; weight decay rate.}

\item{beta_1}{Numeric; parameter for Adam.}

\item{beta_2}{Numeric; parameter for Adam.}

\item{epsilon}{Numeric; a tiny number to put a cap on update size by avoiding
dividing by even smaller numbers.}

\item{exclude_from_weight_decay}{Character; list of parameter names to
exclude from weight decay.}

\item{name}{Character; the name of the constructed object.}
}
\value{
An object of class "AdamWeightDecayOptimizer", which is a (hacky)
  modification of the tf.train.Optimizer class.
}
\description{
A basic Adam optimizer that includes "correct" L2 weight decay.
}
\details{
Inherits from class tf.train.Optimizer.
\url{https://devdocs.io/tensorflow~python/tf/train/optimizer}
}
\examples{
\dontrun{
with(tensorflow::tf$variable_scope("examples",
                                   reuse = tensorflow::tf$AUTO_REUSE),
     {
       optimizer <- AdamWeightDecayOptimizer(learning_rate = 0.01)
     })
}
}
