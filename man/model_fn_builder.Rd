% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/run_classifier.R
\name{model_fn_builder}
\alias{model_fn_builder}
\title{Define \code{model_fn} closure for \code{TPUEstimator}}
\usage{
model_fn_builder(
  bert_config,
  num_labels,
  init_checkpoint,
  learning_rate,
  num_train_steps,
  num_warmup_steps,
  use_tpu,
  use_one_hot_embeddings
)
}
\arguments{
\item{bert_config}{\code{BertConfig} instance.}

\item{num_labels}{Integer; number of classification labels.}

\item{init_checkpoint}{Character; path to the checkpoint directory, plus
checkpoint name stub (e.g. "bert_model.ckpt"). Path must be absolute and
explicit, starting with "/".}

\item{learning_rate}{Numeric; the learning rate.}

\item{num_train_steps}{Integer; number of steps to train for.}

\item{num_warmup_steps}{Integer; number of steps to use for "warm-up".}

\item{use_tpu}{Logical; whether to use TPU.}

\item{use_one_hot_embeddings}{Logical; whether to use one-hot word embeddings
or tf.embedding_lookup() for the word embeddings.}
}
\value{
\code{model_fn} closure for \code{TPUEstimator}.
}
\description{
Returns \code{model_fn} closure, which is an input to \code{TPUEstimator}.
}
\details{
The \code{model_fn} function takes four parameters: \describe{
\item{features}{A list (or similar structure) that contains objects such as
 \code{input_ids}, \code{input_mask}, \code{segment_ids},  and
 \code{label_ids}. These objects will be inputs to the \code{create_model}
 function.}
\item{labels}{Not used in this function, but presumably we need to
 keep this slot here.}
\item{mode}{Character; value such as "train", "infer",
 or "eval".}
\item{params}{Not used in this function, but presumably we need
 to keep this slot here.}
 }

The output of \code{model_fn} is the result of a
\code{tf$contrib$tpu$TPUEstimatorSpec} call.

This reference may be helpful:
\url{https://tensorflow.rstudio.com/tfestimators/articles/creating_estimators.html}
}
\examples{
\dontrun{
with(tensorflow::tf$variable_scope("examples",
                                   reuse = tensorflow::tf$AUTO_REUSE),
     {
       input_ids <- tensorflow::tf$constant(list(list(31L, 51L, 99L),
                                                 list(15L, 5L, 0L)))

       input_mask <- tensorflow::tf$constant(list(list(1L, 1L, 1L),
                                                  list(1L, 1L, 0L)))
       token_type_ids <- tensorflow::tf$constant(list(list(0L, 0L, 1L),
                                                      list(0L, 2L, 0L)))
       config <- BertConfig(vocab_size = 30522L,
                            hidden_size = 768L,
                            num_hidden_layers = 8L,
                            type_vocab_size = 2L,
                            num_attention_heads = 12L,
                            intermediate_size = 3072L)

       temp_dir <- tempdir()
       init_checkpoint <- file.path(temp_dir,
                                    "BERT_checkpoints",
                                    "uncased_L-12_H-768_A-12",
                                    "bert_model.ckpt")

       example_mod_fn <- model_fn_builder(bert_config = config,
                                          num_labels = 2L,
                                          init_checkpoint = init_checkpoint,
                                          learning_rate = 0.01,
                                          num_train_steps = 20L,
                                          num_warmup_steps = 10L,
                                          use_tpu = FALSE,
                                          use_one_hot_embeddings = FALSE)
     }
)
}
}
