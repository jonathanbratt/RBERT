% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tf2-layer_proj_add_norm.R
\name{custom_layer_proj_add_norm}
\alias{custom_layer_proj_add_norm}
\title{Custom Layer: Project, Add, Normalize}
\usage{
custom_layer_proj_add_norm(
  object,
  name = NULL,
  trainable = NULL,
  param_list = list(),
  ...
)
}
\arguments{
\item{object}{Model or layer object.}

\item{name}{Character; An optional name for the layer. Must be unique in a
model.}

\item{trainable}{Logical; whether the layer weights will be updated during
training.}

\item{param_list}{A named list of parameter values used in defining the
layer. Some parameters for the BERT model are:
\describe{
\item{\code{num_layers}}{Integer; the number of attention layers to create.}
\item{\code{num_heads}}{Integer; the number of attention heads per layer.}
\item{\code{hidden_size}}{Integer; the size of the embedding vectors.}
\item{\code{hidden_dropout}}{Dropout probability to apply to dense layers...}
\item{\code{attention_dropout}}{Dropout probability to apply to attention... }
\item{\code{intermediate_size}}{Integer; size of dense layers after self
attention mechanism.}
\item{\code{intermediate_activation}}{Character; activation function to use with
dense layers... }
\item{\code{vocab_size}}{Integer; number of tokens in vocabulary.}
\item{\code{token_type_vocab_size}}{Integer; number of input segments that the
model will recognize. (Two for BERT models.) }
\item{\code{max_position_embeddings}}{Integer; maximum number of tokens in
input.}
\item{\code{embedding_size}}{(Used for ALBERT; NULL for BERT.) }
\item{\code{shared_layer}}{(TRUE for ALBERT, FALSE for BERT.) }
}}
}
\description{
Create a layer that, given two input layers, applies a dense layer projection
(followed by dropout) to the first input, then adds the second (as a
residual) and normalizes the sum.
}
