% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenization.R
\name{tokenize_text}
\alias{tokenize_text}
\title{Tokenize Text with Word Pieces}
\usage{
tokenize_text(
  text,
  ckpt_dir = NULL,
  vocab_file = find_vocab(ckpt_dir),
  include_special = TRUE
)
}
\arguments{
\item{text}{Character vector; text to tokenize.}

\item{ckpt_dir}{Character; path to checkpoint directory. If specified,
\code{vocab_file} defaults to standard filename within \code{ckpt_dir}.}

\item{vocab_file}{Character; path to vocabulary file. File is assumed to be a
text file, with one token per line, with the line number corresponding to
the index of that token in the vocabulary.}

\item{include_special}{Logical; whether to add the special tokens "[CLS]" (at
the beginning) and "[SEP]" (at the end) of the token list.}
}
\value{
A list of character vectors, giving the tokenization of the input
  text.
}
\description{
Given some text and a word piece vocabulary, tokenizes the text. This is
primarily a tool for quickly checking the tokenization of a piece of text.
}
\examples{
\dontrun{
BERT_PRETRAINED_DIR <- download_BERT_checkpoint("bert_base_uncased")
 tokens <- tokenize_text(text = c("Who doesn't like tacos?", "Not me!"),
                         ckpt_dir = BERT_PRETRAINED_DIR)
}
}
