% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenization.R
\name{tokenize_text}
\alias{tokenize_text}
\title{Tokenize Text with Word Pieces}
\usage{
tokenize_text(
  text,
  ckpt_dir = NULL,
  vocab_file = find_vocab(ckpt_dir),
  include_special = TRUE
)
}
\arguments{
\item{text}{Character vector; text to tokenize.}

\item{ckpt_dir}{Character; path to checkpoint directory. If specified, any
other checkpoint files required by this function (\code{vocab_file},
\code{bert_config_file}, or \code{init_checkpoint}) will default to
standard filenames within \code{ckpt_dir}.}

\item{vocab_file}{path to vocabulary file. File is assumed to be a text file,
with one token per line, with the line number corresponding to the index of
that token in the vocabulary.}

\item{include_special}{Logical; whether to add the special tokens "[CLS]" (at
the beginning) and "[SEP]" (at the end) of the token list.}
}
\value{
A list of character vectors, giving the tokenization of the input
  text.
}
\description{
Given some text and a word piece vocabulary, tokenizes the text. This is
primarily a tool for quickly checking the tokenization of a piece of text.
}
\examples{
\dontrun{
BERT_PRETRAINED_DIR <- download_BERT_checkpoint("bert_base_uncased")
 tokens <- tokenize_text(text = c("Who doesn't like tacos?", "Not me!"),
                         ckpt_dir = BERT_PRETRAINED_DIR)
}
}
