% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tf2-layer_attention.R
\name{custom_layer_attention}
\alias{custom_layer_attention}
\title{Custom Layer: Attention}
\usage{
custom_layer_attention(
  object,
  name = NULL,
  trainable = NULL,
  param_list = list(),
  ...
)
}
\arguments{
\item{object}{Model or layer object.}

\item{name}{Character; An optional name for the layer. Must be unique in a
model.}

\item{trainable}{Logical; whether the layer weights will be updated during
training.}

\item{param_list}{A named list of parameter values used in defining the
layer. Some parameters for the BERT model are:
\describe{
\item{\code{num_layers}}{Integer; the number of attention layers to create.}
\item{\code{num_heads}}{Integer; the number of attention heads per layer.}
\item{\code{hidden_size}}{Integer; the size of the embedding vectors.}
\item{\code{hidden_dropout}}{Dropout probability to apply to dense layers...}
\item{\code{attention_dropout}}{Dropout probability to apply to attention... }
\item{\code{intermediate_size}}{Integer; size of dense layers after self
attention mechanism.}
\item{\code{intermediate_activation}}{Character; activation function to use with
dense layers... }
\item{\code{vocab_size}}{Integer; number of tokens in vocabulary.}
\item{\code{token_type_vocab_size}}{Integer; number of input segments that the
model will recognize. (Two for BERT models.) }
\item{\code{max_position_embeddings}}{Integer; maximum number of tokens in
input.}
\item{\code{embedding_size}}{(Used for ALBERT; NULL for BERT.) }
\item{\code{shared_layer}}{(TRUE for ALBERT, FALSE for BERT.) }
}}
}
\description{
Create first part of self attention layer. Takes as input an embeddings layer
(could be the output of a previous attention layer), performs self attention,
adds input layer back via residual connection, and applies layer
normalization.
}
\details{
In an encoder, this layer is typically followed by an intermediate dense
layer, with a redsidual connection before another layer normalization.

Note that this layer implements more of the attention mechanism than
does \code{keras::layer_attention}.
}
