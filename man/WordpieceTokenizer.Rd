% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenization.R
\name{WordpieceTokenizer}
\alias{WordpieceTokenizer}
\title{Construct objects of WordpieceTokenizer class.}
\usage{
WordpieceTokenizer(vocab, unk_token = "[UNK]", max_input_chars_per_word = 200)
}
\arguments{
\item{vocab}{Recognized vocabulary tokens, as a named integer vector. (Name
is token, value is index.)}

\item{unk_token}{Token to use for unknown words.}

\item{max_input_chars_per_word}{Length of longest word we will recognize.}
}
\value{
an object of class WordpieceTokenizer
}
\description{
(I'm not sure that this object-based approach is best for R implementation,
but for now just trying to reproduce python functionality.)
}
\details{
Has method: tokenize.WordpieceTokenizer()
}
\examples{
\dontrun{
vocab <- load_vocab(vocab_file = "vocab.txt")
wp_tokenizer <- WordpieceTokenizer(vocab)
}
}
