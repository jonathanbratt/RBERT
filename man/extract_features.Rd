% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/extract_features.R
\name{extract_features}
\alias{extract_features}
\title{Extract output features from BERT}
\usage{
extract_features(examples, vocab_file, bert_config_file, init_checkpoint,
  output_file = NULL, max_seq_length = 128L, layer_indexes = -4:-1,
  use_one_hot_embeddings = FALSE, batch_size = 2L,
  features = c("output", "attention"))
}
\arguments{
\item{examples}{List of \code{InputExample_EF}s to convert.}

\item{vocab_file}{path to vocabulary file. File is assumed to be a text file,
with one token per line, with the line number corresponding to the index of
that token in the vocabulary.}

\item{bert_config_file}{Character; the path to a json config file.}

\item{init_checkpoint}{Character; path to the checkpoint directory, plus
checkpoint name stub (e.g. "bert_model.ckpt"). Path must be absolute and
explicit, starting with "/".}

\item{output_file}{(optional) Character; file path (stub) for writing output
to.}

\item{max_seq_length}{Integer; the maximum number of tokens that will be
considered together.}

\item{layer_indexes}{Integer vector; indexes (positive, or negative counting
back from the end) indicating which layers to extract as "output features".
The "zeroth" layer embeddings are the input embeddings vectors to the first
layer.}

\item{use_one_hot_embeddings}{Logical; whether to use one-hot word embeddings
or tf.embedding_lookup() for the word embeddings.}

\item{batch_size}{Integer; how many examples to process per batch.}

\item{features}{Character; whether to return "output" (layer outputs, the
default), "attention" (attention probabilities), or both.}
}
\value{
A list with elements "output" (the layer outputs as a tibble),
  "attention" (the attention weights as a tibble), or both.
}
\description{
Given example sentences (as a list of \code{InputExample_EF}s), apply an
existing BERT model and capture certain output layers. (These could
potentially be used as features in downstream tasks.)
}
\examples{
\dontrun{
BERT_PRETRAINED_DIR <- download_BERT_checkpoint("bert_base_uncased")
vocab_file <- file.path(BERT_PRETRAINED_DIR, 'vocab.txt')
init_checkpoint <- file.path(BERT_PRETRAINED_DIR, 'bert_model.ckpt')
bert_config_file <- file.path(BERT_PRETRAINED_DIR, 'bert_config.json')
examples <- list(InputExample_EF(unique_id = 1,
                                  text_a = "I saw the branch on the bank."),
                 InputExample_EF(unique_id = 2,
                                  text_a = "I saw the branch of the bank."))
feats <- extract_features(examples = examples,
                          vocab_file = vocab_file,
                          bert_config_file = bert_config_file,
                          init_checkpoint = init_checkpoint,
                          batch_size = 2L)
}
}
