% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/modeling.R
\name{BertConfig}
\alias{BertConfig}
\title{Construct objects of BertConfig class}
\usage{
BertConfig(
  vocab_size,
  hidden_size = 768L,
  num_hidden_layers = 12L,
  num_attention_heads = 12L,
  intermediate_size = 3072L,
  hidden_act = "gelu",
  hidden_dropout_prob = 0.1,
  attention_probs_dropout_prob = 0.1,
  max_position_embeddings = 512L,
  type_vocab_size = 16L,
  initializer_range = 0.02
)
}
\arguments{
\item{vocab_size}{Integer; vocabulary size of \code{inputs_ids} in
\code{BertModel}.}

\item{hidden_size}{Integer; size of the encoder layers and the pooler layer.}

\item{num_hidden_layers}{Integer; number of hidden layers in the Transformer
encoder.}

\item{num_attention_heads}{Integer; number of attention heads for each
attention layer in the Transformer encoder.}

\item{intermediate_size}{Integer; the size of the "intermediate" (i.e.,
feed-forward) layer in the Transformer encoder.}

\item{hidden_act}{The non-linear activation function (function or string) in
the encoder and pooler.}

\item{hidden_dropout_prob}{Numeric; the dropout probability for all fully
connected layers in the embeddings, encoder, and pooler.}

\item{attention_probs_dropout_prob}{Numeric; the dropout ratio for the
attention probabilities.}

\item{max_position_embeddings}{Integer; the maximum sequence length that this
model might ever be used with. Typically set this to something large just
in case (e.g., 512 or 1024 or 2048).}

\item{type_vocab_size}{Integer; the vocabulary size of the
\code{token_type_ids} passed into \code{BertModel}.}

\item{initializer_range}{Numeric; the stdev of the
truncated_normal_initializer for initializing all weight matrices.}
}
\value{
An object of class BertConfig
}
\description{
Given a set of values as parameter inputs, construct a BertConfig object with
those values.
}
\examples{
\dontrun{
BertConfig(vocab_size = 30522L)
}
}
